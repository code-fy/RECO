{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.insert(0, os.path.join(BASE_DIR))\n",
    "from offline import SparkSessionBase\n",
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainWord2VecModel(SparkSessionBase):\n",
    "    \n",
    "    SPARK_APP_NAME = \"Word2Vec\"\n",
    "    ENABLE_HIVE_SUPPORT = True\n",
    "    SPARK_EXECUTOR_MEMORY = \"4g\"\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.spark = self._create_spark_session()\n",
    "        \n",
    "w2v = TrainWord2VecModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(partition):\n",
    "    import os\n",
    "    import re\n",
    "    import jieba\n",
    "    import jieba.analyse\n",
    "    import jieba.posseg as pseg\n",
    "    import codecs\n",
    "    \n",
    "    abspath = \"/Users/hycao/text\"\n",
    "    \n",
    "    userDict_path = os.path.join(abspath, \"ITKeywords.txt\")\n",
    "    jieba.load_userdict(userDict_path)\n",
    "    stopwords_path = os.path.join(abspath, \"stopwords.txt\")\n",
    "    \n",
    "    def get_stopwords_list():\n",
    "        stopwords_list = [i.strip() for i in codecs.open(stopwords_path).readlines()]\n",
    "        return stopwords_list\n",
    "    \n",
    "    stopwords_list = get_stopwords_list()\n",
    "    \n",
    "    def cut_sentence(sentence):\n",
    "        seg_list = pseg.lcut(sentence)\n",
    "        seg_list = [i for i in seg_list if i.flag not in stopwords_list]\n",
    "        filtered_words_list = []\n",
    "        for seg in seg_list:\n",
    "            if len(seg.word) <= 1:\n",
    "                continue\n",
    "            elif seg.flag == \"eng\":\n",
    "                if len(seg.word) <= 2:\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    filtered_words_list.append(seg.word)\n",
    "            elif seg.flag.startswith(\"n\"):\n",
    "                filtered_words_list.append(seg.word)\n",
    "            elif seg.flag in [\"X\", \"eng\"]:\n",
    "                filtered_words_list.append(seg.word)\n",
    "        return filtered_words_list\n",
    "    \n",
    "    for row in partition:\n",
    "        sentence = re.sub(\"<.*?>\", \"\", row.sentence)\n",
    "        words = cut_sentence(sentence)\n",
    "        yield row.article_id, row.channel_id, words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.spark.sql(\"use fytang\")\n",
    "article_data = w2v.spark.sql(\"select * from article_data where channel_id = 18\")\n",
    "words_df = article_data.rdd.mapPartitions(segmentation).toDF(['article_id', 'channel_id', 'words'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = Word2Vec(vectorSize=100, inputCol='words', outputCol='model')\n",
    "wv_model = wv.fit(words_df)\n",
    "wv_model.save(\"hdfs://localhost:9000/fytang/models/test.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2VecModel\n",
    "word2vec = Word2VecModel.load(\"hdfs://localhost:9000/fytang/models/test.word2vec\")\n",
    "vectors = word2vec.getVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      word|              vector|\n",
      "+----------+--------------------+\n",
      "|        广义|[0.14468304812908...|\n",
      "|        伙伴|[0.00758749945089...|\n",
      "|        箭头|[0.17873702943325...|\n",
      "|      COCO|[0.11621176451444...|\n",
      "|        拜拜|[0.00776594784110...|\n",
      "|  quotient|[0.04783434420824...|\n",
      "|        货币|[0.07925040274858...|\n",
      "|        人物|[0.21718211472034...|\n",
      "|       wsy|[0.00973777379840...|\n",
      "|fromParams|[0.05492971837520...|\n",
      "|ershoufang|[-0.0702918618917...|\n",
      "|        热身|[0.10387352854013...|\n",
      "|    breaks|[0.01752104423940...|\n",
      "|      marr|[-0.0695709511637...|\n",
      "|       可靠性|[-0.2670255005359...|\n",
      "|      测试代码|[0.20592857897281...|\n",
      "|       pys|[0.00484006293118...|\n",
      "|       dns|[-0.0224121324717...|\n",
      "|   frmongo|[0.00727079855278...|\n",
      "|       ROW|[-0.1352243125438...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+--------+\n",
      "|article_id|channel_id|keyword|   tfidf|\n",
      "+----------+----------+-------+--------+\n",
      "|     98319|        17|    var| 20.6079|\n",
      "|     98323|        17|    var|  7.4938|\n",
      "|     98326|        17|    var|104.9128|\n",
      "|     98344|        17|    var|  5.6203|\n",
      "|     98359|        17|    var| 69.3174|\n",
      "|     98360|        17|    var|  9.3672|\n",
      "|     98392|        17|    var| 14.9875|\n",
      "|     98393|        17|    var|155.4958|\n",
      "|     98406|        17|    var| 11.2407|\n",
      "|     98419|        17|    var| 59.9502|\n",
      "|     98442|        17|    var| 18.7344|\n",
      "|     98445|        17|    var| 37.4689|\n",
      "|     98512|        17|    var| 29.9751|\n",
      "|     98544|        17|    var|  5.6203|\n",
      "|     98545|        17|    var| 22.4813|\n",
      "|     98548|        17|    var| 71.1909|\n",
      "|     98599|        17|    var| 11.2407|\n",
      "|     98609|        17|    var| 18.7344|\n",
      "|     98642|        17|    var|  67.444|\n",
      "|     98648|        15|    var| 20.6079|\n",
      "+----------+----------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "article_profile = w2v.spark.sql(\"select * from tfidf_keywords_values\")\n",
    "article_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_profile.registerTempTable(\"incremental\")\n",
    "# keyword_weight = w2v.spark.sql(\"select article_id,channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "_article_profile = article_profile.join(vectors,vectors.word == article_profile.keyword,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+--------+----+--------------------+\n",
      "|article_id|channel_id|keyword|   tfidf|word|              vector|\n",
      "+----------+----------+-------+--------+----+--------------------+\n",
      "|     98319|        17|    var| 20.6079| var|[0.07167968153953...|\n",
      "|     98323|        17|    var|  7.4938| var|[0.07167968153953...|\n",
      "|     98326|        17|    var|104.9128| var|[0.07167968153953...|\n",
      "|     98344|        17|    var|  5.6203| var|[0.07167968153953...|\n",
      "|     98359|        17|    var| 69.3174| var|[0.07167968153953...|\n",
      "|     98360|        17|    var|  9.3672| var|[0.07167968153953...|\n",
      "|     98392|        17|    var| 14.9875| var|[0.07167968153953...|\n",
      "|     98393|        17|    var|155.4958| var|[0.07167968153953...|\n",
      "|     98406|        17|    var| 11.2407| var|[0.07167968153953...|\n",
      "|     98419|        17|    var| 59.9502| var|[0.07167968153953...|\n",
      "|     98442|        17|    var| 18.7344| var|[0.07167968153953...|\n",
      "|     98445|        17|    var| 37.4689| var|[0.07167968153953...|\n",
      "|     98512|        17|    var| 29.9751| var|[0.07167968153953...|\n",
      "|     98544|        17|    var|  5.6203| var|[0.07167968153953...|\n",
      "|     98545|        17|    var| 22.4813| var|[0.07167968153953...|\n",
      "|     98548|        17|    var| 71.1909| var|[0.07167968153953...|\n",
      "|     98599|        17|    var| 11.2407| var|[0.07167968153953...|\n",
      "|     98609|        17|    var| 18.7344| var|[0.07167968153953...|\n",
      "|     98642|        17|    var|  67.444| var|[0.07167968153953...|\n",
      "|     98648|        15|    var| 20.6079| var|[0.07167968153953...|\n",
      "+----------+----------+-------+--------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_article_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|article_id|channel_id|              vector|\n",
      "+----------+----------+--------------------+\n",
      "|     98319|        17|[0.07167968153953...|\n",
      "|     98323|        17|[0.07167968153953...|\n",
      "|     98326|        17|[0.07167968153953...|\n",
      "|     98344|        17|[0.07167968153953...|\n",
      "|     98359|        17|[0.07167968153953...|\n",
      "|     98360|        17|[0.07167968153953...|\n",
      "|     98392|        17|[0.07167968153953...|\n",
      "|     98393|        17|[0.07167968153953...|\n",
      "|     98406|        17|[0.07167968153953...|\n",
      "|     98419|        17|[0.07167968153953...|\n",
      "|     98442|        17|[0.07167968153953...|\n",
      "|     98445|        17|[0.07167968153953...|\n",
      "|     98512|        17|[0.07167968153953...|\n",
      "|     98544|        17|[0.07167968153953...|\n",
      "|     98545|        17|[0.07167968153953...|\n",
      "|     98548|        17|[0.07167968153953...|\n",
      "|     98599|        17|[0.07167968153953...|\n",
      "|     98609|        17|[0.07167968153953...|\n",
      "|     98642|        17|[0.07167968153953...|\n",
      "|     98648|        15|[0.07167968153953...|\n",
      "+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_article_profile = _article_profile.select(['article_id', 'channel_id', 'vector'])\n",
    "_article_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "_article_profile.registerTempTable(\"temptable\")\n",
    "articlevector = w2v.spark.sql(\"select article_id, max(channel_id) channel_id, collect_set(vector) articlevecter from temptable group by article_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|article_id|channel_id|       articlevecter|\n",
      "+----------+----------+--------------------+\n",
      "|       148|        17|[[-0.465461105108...|\n",
      "|       463|         1|[[-0.395194649696...|\n",
      "|       471|        17|[[-0.040295206010...|\n",
      "|       496|        11|[[-0.090842999517...|\n",
      "|       833|         1|[[-0.235992595553...|\n",
      "|      1088|         1|[[-0.013495572842...|\n",
      "|      1238|        11|[[0.4386835694313...|\n",
      "|      1342|         6|[[-0.036753281950...|\n",
      "|      1580|         6|[[-0.066306084394...|\n",
      "|      1591|        17|[[0.2542469501495...|\n",
      "|      1645|         6|[[-0.390882134437...|\n",
      "|      1829|        11|[[0.1422660946846...|\n",
      "|      1959|        11|[[0.0840188488364...|\n",
      "|      2122|        17|[[-0.013495572842...|\n",
      "|      2142|        11|[[0.0012916581472...|\n",
      "|      2366|        13|[[-0.047671638429...|\n",
      "|      2659|        17|[[-0.040295206010...|\n",
      "|      2866|        17|[[0.1863891184329...|\n",
      "|      3175|         6|[[-0.327978760004...|\n",
      "|      3749|        15|[[-0.066306084394...|\n",
      "+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articlevector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vector(row):\n",
    "    x = 0\n",
    "    for v in row.articlevecter:\n",
    "        x += v\n",
    "    return row.article_id, row.channel_id, x/len(row.articlevecter)\n",
    "articlevector = articlevector.rdd.map(avg_vector).toDF(['article_id', 'channel_id', 'articlevector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
